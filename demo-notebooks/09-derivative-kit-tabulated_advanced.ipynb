{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DerivKit: DerivativeKit — Tabulated derivatives (advanced)\n",
    "\n",
    "### Summary\n",
    "This notebook demonstrates estimating **first derivatives from noisy tabulated data** using `DerivativeKit`\n",
    "in *tabulated mode*. We use a toy problem with an analytic truth:\n",
    "\n",
    "- underlying function: $f(x)=\\sin(x)$\n",
    "- truth derivative: $f'(x)=\\cos(x)$\n",
    "\n",
    "We add Gaussian noise to the sampled table and compare two derivative methods across many evaluation points:\n",
    "- **finite differences + Ridders extrapolation**\n",
    "- **adaptive local fit**\n",
    "\n",
    "### Usage\n",
    "If you prefer to run the same example as a script:\n",
    "\n",
    "```bash\n",
    "$ python demo-scripts/09-derivative-kit-tabulated_advanced.py\n",
    "```\n",
    "\n",
    "### What it does\n",
    "- Builds a noisy tabulated dataset (optionally with x-jitter).\n",
    "- Evaluates $f'(x)$ on an interior grid of $x_0$ values.\n",
    "- Compares methods to the analytic truth and prints accuracy metrics (RMSE/MAE/MaxAE).\n",
    "- Produces three plots:\n",
    "  1. noisy table vs truth\n",
    "  2. derivative estimates (with internal errors when available) vs truth\n",
    "  3. method-reported internal error $\\widehat{\\sigma}_{f'}(x)$ vs $x$\n",
    "\n",
    "### Notes\n",
    "- The “internal error” is whatever the method reports when `return_error=True`; it is method-dependent and\n",
    "  not necessarily a calibrated statistical 1σ uncertainty.\n",
    "\n",
    "### Requirements\n",
    "- `derivkit` installed and importable in your Python environment.\n",
    "- `matplotlib` installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Make repo root importable so `utils.style` works when running from demo-scripts/\n",
    "# If you run this notebook from the derivkit-demos repo root, this should be fine.\n",
    "sys.path.insert(0, str(Path.cwd().resolve().parents[0]))\n",
    "\n",
    "from derivkit.derivative_kit import DerivativeKit\n",
    "from derivkit.derivatives.tabulated_model.one_d import Tabulated1DModel\n",
    "\n",
    "# Optional styling (if available in your demos repo)\n",
    "try:\n",
    "    from utils.style import DEFAULT_COLORS, apply_plot_style\n",
    "except Exception:\n",
    "    DEFAULT_COLORS = {\"red\": None, \"yellow\": None, \"blue\": None}\n",
    "    def apply_plot_style():\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_func(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Retruns a sine function.\"\"\"\n",
    "    return np.sin(x)\n",
    "\n",
    "def df_truth(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns the analytic derivative of the sine function (a cosine).\"\"\"\n",
    "    return np.cos(x)\n",
    "\n",
    "def metrics(estimate: np.ndarray, truth: np.ndarray) -> dict[str, float]:\n",
    "    \"\"\"Compute RMSE, MAE, and MaxAE between estimate and truth arrays.\n",
    "\n",
    "    RMSE is the root-mean-square error, computed as sqrt(mean((estimate - truth)^2)).\n",
    "    MAE is the mean absolute error, computed as mean(|estimate - truth|).\n",
    "    MaxAE is the maximum absolute error, computed as max(|estimate - truth|).\n",
    "    \"\"\"\n",
    "    m = np.isfinite(estimate) & np.isfinite(truth)\n",
    "    if not np.any(m):\n",
    "        return {\"rmse\": float(\"nan\"), \"mae\": float(\"nan\"), \"maxae\": float(\"nan\")}\n",
    "    err = estimate[m] - truth[m]\n",
    "    ae = np.abs(err)\n",
    "    return {\n",
    "        \"rmse\": float(np.sqrt(np.mean(err * err))),\n",
    "        \"mae\": float(np.mean(ae)),\n",
    "        \"maxae\": float(np.max(ae)),\n",
    "    }\n",
    "\n",
    "def tex_sci(x: float, sig: int = 2) -> str:\n",
    "    \"\"\"Format a float in LaTeX scientific notation with given significant figures.\"\"\"\n",
    "    if not np.isfinite(x) or x == 0.0:\n",
    "        return r\"0\"\n",
    "    sgn = \"-\" if x < 0 else \"\"\n",
    "    ax = abs(x)\n",
    "    exp = int(np.floor(np.log10(ax)))\n",
    "    mant = ax / (10.0 ** exp)\n",
    "    if exp == 0:\n",
    "        return rf\"{sgn}{mant:.{sig}f}\"\n",
    "    return rf\"{sgn}{mant:.{sig}f}\\times10^{{{exp}}}\"\n",
    "\n",
    "def compute_method_over_grid(\n",
    "    x0_grid: np.ndarray,\n",
    "    model: Tabulated1DModel,\n",
    "    method_name: str,\n",
    "    **extra_kwargs: Any,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"A wrapper to compute first derivatives over a grid of x0 values.\n",
    "\n",
    "    This method uses DerivativeKit to compute the first derivative\n",
    "    of the tabulated data at each x0 in x0_grid using the specified method\n",
    "    of DerivativeKit.\n",
    "    \"\"\"\n",
    "    slopes = np.empty_like(x0_grid, dtype=float)\n",
    "    errs = np.full_like(x0_grid, np.nan, dtype=float)\n",
    "\n",
    "    for i, x0 in enumerate(x0_grid):\n",
    "        dk = DerivativeKit(function=model, x0=float(x0))\n",
    "        try:\n",
    "            val, err = dk.differentiate(\n",
    "                method=method_name,\n",
    "                order=1,\n",
    "                return_error=True,\n",
    "                **extra_kwargs,\n",
    "            )\n",
    "            slopes[i] = float(np.asarray(val).reshape(-1)[0])\n",
    "            errs[i] = float(np.asarray(err).reshape(-1)[0])\n",
    "        except TypeError:\n",
    "            # method may not support return_error\n",
    "            try:\n",
    "                val = dk.differentiate(method=method_name, order=1, **extra_kwargs)\n",
    "                slopes[i] = float(np.asarray(val).reshape(-1)[0])\n",
    "                errs[i] = np.nan\n",
    "            except Exception:\n",
    "                slopes[i] = np.nan\n",
    "                errs[i] = np.nan\n",
    "        except Exception:\n",
    "            slopes[i] = np.nan\n",
    "            errs[i] = np.nan\n",
    "\n",
    "    return slopes, errs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1) Build a noisy tabulated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_plot_style()  # apply DerivkKit plot style\n",
    "\n",
    "rng = np.random.default_rng(42)  # random number generator for reproducibility, fixed seed at 42\n",
    "\n",
    "n_tab = 70  # number of tabulated points\n",
    "x_tab = np.linspace(0.0, 2.0 * np.pi, n_tab)  # x values on a regular grid\n",
    "y_clean = sin_func(x_tab)  # clean sin(x) samples (perfect function values)\n",
    "\n",
    "y_noise_sigma = 0.05  # a 5% noise level in y_tab\n",
    "x_jitter_sigma = 0.0  # set >0 to enable x-jitter\n",
    "\n",
    "y_noisy = y_clean + rng.normal(0.0, y_noise_sigma, size=y_clean.shape)\n",
    "\n",
    "if x_jitter_sigma > 0:\n",
    "    x_noisy = x_tab + rng.normal(0.0, x_jitter_sigma, size=x_tab.shape)\n",
    "    srt = np.argsort(x_noisy)\n",
    "    x_noisy = x_noisy[srt]\n",
    "    y_noisy = y_noisy[srt]\n",
    "else:\n",
    "    x_noisy = x_tab.copy()\n",
    "\n",
    "model_noisy = Tabulated1DModel(x_noisy, y_noisy, extrapolate=True)\n",
    "\n",
    "x_noisy[:5], y_noisy[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2) Evaluate derivatives across an interior grid of $x_0$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval = 50  # number of evaluation points for derivative estimates\n",
    "# For the interior grid, avoid edges by 0.15 on each side\n",
    "# just to be safe with finite-difference stencils\n",
    "x0 = np.linspace(x_tab.min() + 0.15, x_tab.max() - 0.15, n_eval)\n",
    "truth = df_truth(x0)\n",
    "\n",
    "slopes_fr, errs_fr = compute_method_over_grid(\n",
    "    x0, model_noisy, \"finite\", extrapolation=\"ridders\"\n",
    ")\n",
    "slopes_ad, errs_ad = compute_method_over_grid(\n",
    "    x0, model_noisy, \"adaptive\", n_points=27, spacing=0.25\n",
    ")\n",
    "\n",
    "m_fr = metrics(slopes_fr, truth)\n",
    "m_ad = metrics(slopes_ad, truth)\n",
    "\n",
    "print(\"Derivative accuracy vs truth:\")\n",
    "print(f\"finite (Ridders): RMSE={m_fr['rmse']:.3e}  MAE={m_fr['mae']:.3e}  MaxAE={m_fr['maxae']:.3e}\")\n",
    "print(f\"adaptive        : RMSE={m_ad['rmse']:.3e}  MAE={m_ad['mae']:.3e}  MaxAE={m_ad['maxae']:.3e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3) Plot 1 — noisy function table vs truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors (fallback to matplotlib defaults if style dict not present)\n",
    "c_truth = DEFAULT_COLORS.get(\"red\", None)\n",
    "c_finite = DEFAULT_COLORS.get(\"yellow\", None)\n",
    "\n",
    "x_dense = np.linspace(x_tab.min(), x_tab.max(), 800)\n",
    "y_dense = sin_func(x_dense)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.plot(x_dense, y_dense, lw=2.0, label=rf\"truth $\\sin(x)$\", color=c_truth)\n",
    "ax.plot(\n",
    "    x_noisy,\n",
    "    y_noisy,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.0,\n",
    "    ms=5.0,\n",
    "    label=rf\"$y=\\sin(x)+\\mathcal{{N}}(0,{y_noise_sigma:g}^2)$\",\n",
    "    color=c_finite,\n",
    "    marker=\"o\",\n",
    "    mfc=\"none\",\n",
    "    mec=c_finite,\n",
    "    mew=1.2,\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$f(x)$\")\n",
    "ax.set_title(\"Exact function and noisy samples\")\n",
    "ax.legend(frameon=False, loc=1)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4) Plot 2 — derivative estimates (with internal errors when available) vs truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_adaptive = DEFAULT_COLORS.get(\"blue\", None)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.plot(x0, truth, lw=2.5, label=fr\"truth $\\cos(x)$\", color=c_truth)\n",
    "\n",
    "ax.errorbar(\n",
    "    x0,\n",
    "    slopes_fr,\n",
    "    yerr=errs_fr,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.0,\n",
    "    capsize=2,\n",
    "    label=\"finite (Ridders)\",\n",
    "    color=c_finite,\n",
    "    marker=\"o\",\n",
    "    mfc=\"none\",\n",
    "    mec=c_finite,\n",
    "    mew=1.2,\n",
    "    markersize=5,\n",
    ")\n",
    "\n",
    "ax.errorbar(\n",
    "    x0,\n",
    "    slopes_ad,\n",
    "    yerr=errs_ad,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.0,\n",
    "    capsize=2,\n",
    "    label=\"adaptive\",\n",
    "    color=c_adaptive,\n",
    "    marker=\"o\",\n",
    "    mfc=\"none\",\n",
    "    mec=c_adaptive,\n",
    "    mew=1.2,\n",
    "    markersize=5,\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(r\"$f'(x)$\")\n",
    "ax.set_title(\"Noisy tabulated derivative estimates vs truth\")\n",
    "ax.legend(frameon=False)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5) Plot 3 — method-reported internal error vs $x$\n",
    "\n",
    "Legend includes RMSE and MAE (computed vs truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_fr = (\n",
    "    r\"$\\mathrm{finite\\ (Ridders)}:$\"\n",
    "    \"\\n\"\n",
    "    + rf\"$\\mathrm{{RMSE}}={tex_sci(m_fr['rmse'])}$\"\n",
    "    \"\\n\"\n",
    "    + rf\"$\\mathrm{{MAE}}={tex_sci(m_fr['mae'])}$\"\n",
    ")\n",
    "\n",
    "lab_ad = (\n",
    "    r\"$\\mathrm{adaptive}:$\"\n",
    "    \"\\n\"\n",
    "    + rf\"$\\mathrm{{RMSE}}={tex_sci(m_ad['rmse'])}$\"\n",
    "    \"\\n\"\n",
    "    + rf\"$\\mathrm{{MAE}}={tex_sci(m_ad['mae'])}$\"\n",
    ")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.semilogy(\n",
    "    x0,\n",
    "    errs_fr,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.0,\n",
    "    label=lab_fr,\n",
    "    color=c_finite,\n",
    "    marker=\"o\",\n",
    "    mfc=\"none\",\n",
    "    mec=c_finite,\n",
    "    mew=1.2,\n",
    "    markersize=5,\n",
    ")\n",
    "\n",
    "ax.semilogy(\n",
    "    x0,\n",
    "    errs_ad,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1.0,\n",
    "    label=lab_ad,\n",
    "    color=c_adaptive,\n",
    "    marker=\"o\",\n",
    "    mfc=\"none\",\n",
    "    mec=c_adaptive,\n",
    "    mew=1.2,\n",
    "    markersize=5,\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(r\"$\\widehat{\\sigma}_{f'}(x)$\")\n",
    "ax.set_title(\"Method-reported internal error vs $x$\")\n",
    "ax.legend(frameon=True, fontsize=11, loc=4)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ccl3)",
   "language": "python",
   "name": "ccl3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
