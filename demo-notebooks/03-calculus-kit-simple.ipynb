{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# DerivKit: CalculusKit — Simple Analytic Function Demo (Gradient, Jacobian, Hessian)\n",
    "\n",
    "### Summary\n",
    "This notebook demonstrates how to use :class:`CalculusKit` to compute the **gradient** and **Hessian** of a scalar-valued function, and the **Jacobian** of a vector-valued function, then compare them against analytic results at a chosen point.\n",
    "\n",
    "We use two well-known test cases to verify numerical accuracy.\n",
    "\n",
    "#### Functions\n",
    "\n",
    "**Scalar:** Rosenbrock\n",
    "  $f(x, y) = (a - x)^2 + b (y - x^2)^2$ with analytic gradient $\\nabla f$ and Hessian $H$.\n",
    "\n",
    "**Vector:**\n",
    "  $g(x_1, x_2) = [x_1^2, \\sin(x_2), x_1 x_2]$ with analytic Jacobian $J$.\n",
    "\n",
    "### Usage\n",
    "If you prefer to run this as a standalone script, it lives in `demo-scripts/03-calculus-kit-simple.py`.\n",
    "\n",
    "```bash\n",
    "$ python demo-scripts/03-calculus-kit-simple.py\n",
    "```\n",
    "\n",
    "### What it does\n",
    "- Defines both scalar and vector functions and their analytic derivatives.\n",
    "- Constructs a `:class:CalculusKit` with $x_0 = [0.7, -1.2]$.\n",
    "- Computes:\n",
    "  - $\\nabla f$ (gradient) and $H$ (Hessian) using the adaptive backend.\n",
    "  - $J$ (Jacobian) for the vector function using the adaptive backend.\n",
    "- Prints both numerical and analytic values and compares them via:\n",
    "  - maximum absolute difference $(\\max |\\Delta|)$, and\n",
    "   - Euclidean norm $( || \\Delta||_2)$.\n",
    "\n",
    "### Notes\n",
    "- You can pass backend options such as `method=\"finite\"` or other keyword arguments through the `CalculusKit` method calls.\n",
    "- The adaptive backend automatically selects a local grid size for stable polynomial fits.\n",
    "- All derivative routines support multi-dimensional inputs and arbitrary scalar/vector outputs.\n",
    "- Use small test functions (like Rosenbrock) to verify correctness before applying to complex models.\n",
    "\n",
    "### Requirements\n",
    "- `derivkit` installed and importable in your Python environment.\n",
    "- Optional: `numpy` for vector operations and `matplotlib` if you wish to visualize gradients or Hessians.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from derivkit.calculus_kit import CalculusKit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Test functions and analytic references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x: np.ndarray, a: float = 1.0, b: float = 100.0) -> float:\n",
    "    \"\"\"Scalar-valued Rosenbrock: f(x,y) = (a - x)^2 + b (y - x^2)^2.\"\"\"\n",
    "    x1, x2 = float(x[0]), float(x[1])\n",
    "    return (a - x1) ** 2 + b * (x2 - x1 ** 2) ** 2\n",
    "\n",
    "\n",
    "def rosenbrock_grad_analytic(x: np.ndarray, a: float = 1.0, b: float = 100.0) -> np.ndarray:\n",
    "    \"\"\"Analytic gradient of Rosenbrock.\"\"\"\n",
    "    x1, x2 = float(x[0]), float(x[1])\n",
    "    dfdx = 2 * (x1 - a) - 4 * b * x1 * (x2 - x1 ** 2)\n",
    "    dfdy = 2 * b * (x2 - x1 ** 2)\n",
    "    return np.array([dfdx, dfdy], dtype=float)\n",
    "\n",
    "\n",
    "def rosenbrock_hess_analytic(x: np.ndarray, a: float = 1.0, b: float = 100.0) -> np.ndarray:\n",
    "    \"\"\"Analytic Hessian of Rosenbrock (independent of a).\"\"\"\n",
    "    x1, x2 = float(x[0]), float(x[1])\n",
    "    dxx = 2 - 4 * b * x2 + 12 * b * x1 ** 2\n",
    "    dxy = -4 * b * x1\n",
    "    dyy = 2 * b\n",
    "    return np.array([[dxx, dxy],\n",
    "                     [dxy, dyy]], dtype=float)\n",
    "\n",
    "\n",
    "def vec_func(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Vector-valued example: g(x1,x2) = [ x1^2,  sin(x2),  x1*x2 ].\"\"\"\n",
    "    x1, x2 = float(x[0]), float(x[1])\n",
    "    return np.array([x1**2, np.sin(x2), x1 * x2], dtype=float)\n",
    "\n",
    "\n",
    "def vec_func_jac_analytic(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Analytic Jacobian of vec_func: shape (3 outputs, 2 params).\"\"\"\n",
    "    x1, x2 = float(x[0]), float(x[1])\n",
    "    return np.array([\n",
    "        [2 * x1, 0.0],  # d(x1^2)/dx\n",
    "        [0.0,    np.cos(x2)],  # d(sin x2)/dx\n",
    "        [x2,     x1],  # d(x1*x2)/dx\n",
    "    ], dtype=float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Helpers for printing and error summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(name: str, arr: np.ndarray) -> None:\n",
    "    \"\"\"Pretty-print a named array.\"\"\"\n",
    "    print(f\"{name}:\\n{np.array(arr, dtype=float)}\\n\")\n",
    "\n",
    "def show_delta(name: str, a: np.ndarray, b: np.ndarray) -> None:\n",
    "    \"\"\"Show difference between two arrays with relative error metrics.\"\"\"\n",
    "    a = np.array(a, dtype=float)\n",
    "    b = np.array(b, dtype=float)\n",
    "    diff = a - b\n",
    "    eps = 1e-15\n",
    "    denom = np.maximum(1.0, np.abs(b)) + eps\n",
    "    rel_elem = np.abs(diff) / denom\n",
    "    rel_max = np.max(rel_elem)\n",
    "    rel_rms = np.sqrt(np.mean(rel_elem**2))\n",
    "    print(f\"{name} delta (num - analytic):\")\n",
    "    print(diff)\n",
    "    print(f\"  max rel err = {rel_max:.3e},  rms rel err = {rel_rms:.3e}\")\n",
    "    print(f\"  max|Δ| = {np.max(np.abs(diff)):.3e},  ||Δ||₂ = {np.linalg.norm(diff):.3e}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Run examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation point\n",
    "x0 = np.array([0.7, -1.2], dtype=float)\n",
    "print(\"=== CalculusKit demo at x0 =\", x0, \"===\\n\")\n",
    "\n",
    "# Instantiate CalculusKit for each function\n",
    "calc_rosen = CalculusKit(rosenbrock, x0=x0)\n",
    "calc_vec = CalculusKit(vec_func,   x0=x0)\n",
    "\n",
    "# --- Gradient & Hessian (scalar-valued) ---\n",
    "grad_num = calc_rosen.gradient(method=\"adaptive\")\n",
    "hess_num = calc_rosen.hessian(method=\"adaptive\")\n",
    "\n",
    "grad_ref = rosenbrock_grad_analytic(x0)\n",
    "hess_ref = rosenbrock_hess_analytic(x0)\n",
    "\n",
    "pretty_print(\"∇f (numeric)\", grad_num)\n",
    "pretty_print(\"∇f (analytic)\", grad_ref)\n",
    "show_delta(\"∇f\", grad_num, grad_ref)\n",
    "\n",
    "pretty_print(\"H (numeric)\", hess_num)\n",
    "pretty_print(\"H (analytic)\", hess_ref)\n",
    "show_delta(\"H\", hess_num, hess_ref)\n",
    "\n",
    "# --- Jacobian (vector-valued) ---\n",
    "jac_num = calc_vec.jacobian(method=\"adaptive\")\n",
    "jac_ref = vec_func_jac_analytic(x0)\n",
    "\n",
    "pretty_print(\"J (numeric)\", jac_num)\n",
    "pretty_print(\"J (analytic)\", jac_ref)\n",
    "show_delta(\"J\", jac_num, jac_ref)\n",
    "\n",
    "print(\"Done. ∇ guides, H bends, J translates.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
